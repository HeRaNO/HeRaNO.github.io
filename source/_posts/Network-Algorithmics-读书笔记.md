---
title: Network Algorithmics 读书笔记
date: 2023-02-22 22:16:21
categories: 'Networking'
tags:
	- '笔记'
description: ' '
---

老板让我看两本书：[Network Algorithmics](https://www.elsevier.com/books/network-algorithmics/varghese/978-0-12-809927-8) 和 [High Performance Switches and Routers](https://ieeexplore.ieee.org/book/5236680)，搜了一下发现后一本书比较老，~~并且亚马逊上书评（两条）不怎么好~~，所以先看第一本了。

但是全英的，边看还要边复习英语，虽然也看过翻译过不少英文题，但还是有点崩溃。争取一天看一点，一周一更吧。

实际上中科大开了这门课：[网络算法学](http://staff.ustc.edu.cn/~bhua/)，但是只有前九章，也就是前两个部分，有课件但是没有课程录像什么的。Harvey Mudd College 也开了这门课：[CS181AG](https://www.cs.hmc.edu/~apadmanabhan/cs181ag-F22/)，有课程录像和课件，但是是英文的，还是看书吧……

> 个人评论用引用表达，就像这句话这样。个人观点可能有失偏颇，并且可能修改，请勿引用。

## 前言

网络瓶颈（*network bottlenecks*）是影响网络性能的主要因素之一。有两种瓶颈：资源瓶颈（*resource bottlenecks*）和实现瓶颈（*implementation bottlenecks*）。

资源瓶颈指底层硬件受限，比如处理器速度慢，通讯连接慢等等。但资源瓶颈可以灵活处理，花钱就能变强。

主要问题是实现瓶颈，下层硬件资源甚至是冗余的，上层程序跑得依然很慢。本书关心的就是这种瓶颈，特别是在服务器和路由器上的实现瓶颈。

本书旨在提供在所有网络上都能克服实现瓶颈的技术，并提供能够克服现在和未来网络瓶颈的准则和模型。

网络算法学（*network algorithmics*）是跨学科的，需要体系结构，操作系统，硬件设计和算法知识。网络算法学也是一个系统性的方法。

> 网络算法学这个名字是直译的，但是一些图论算法也被叫做 network algorithmics，我不确定是不是有更好的译名，只是遵从中科大课程的名字。但是可以肯定的是，网络算法学不仅包括算法，还糅合了实现黑魔法，不属于严格意义上的算法学研究领域。个人认为分为两部分：属于算法学研究的领域，和更高阶的算法实现技术（或技巧）。

本质上，本书讨论三件事：

- 基本的网络实现瓶颈
  - PC 或服务器的瓶颈包括：数据复制，控制传输，多路复用，计时器，缓存分配，校验和和协议处理；
  - 互联设备的瓶颈包括：前缀查询和精确查询，包分类，交换，测量实现和权限安全实现。
- 解决新瓶颈的基本原则
- 从基本原则出发，解决特殊瓶颈的方法

### 15 个解决网络瓶颈的原则

> 详细的解释在后面的章节，这里先补个课。

1. 避免明显的浪费（*Avoid obvious waste*）

   - [零拷贝接口](https://en.wikipedia.org/wiki/Zero-copy)（*Zero-copy interfaces*）

     > 目前零拷贝是一个十分通用的技术了，可以看 [netpoll](https://github.com/cloudwego/netpoll) 的实现。

2. 将计算变为时间（*Shift computation in time*）

   > 关于这个原则的翻译我不好说，这个原则和原则 3.3. 的形式一致，但是翻译过来就怪怪的。这个原则主要针对一些可合并的操作，采用一些方式降低处理数据的时间消耗。

   1. 预处理（*Precompute*）

      - 应用设备通道（*Application device channels*）

        > 网上并没有什么介绍，我没听说过也不知道这是什么……

   2. 惰性计算（*Evaluate lazily*）

      - [写时复制](https://en.wikipedia.org/wiki/Copy-on-write)（*Copy-on-write*）

        > 这也是一个比较通用的技术了，netpoll 的实现中也有，在无锁编程里也有这样的思想。其实和线段树 lazy tag 一样，用的时候再传标记。不过也是看场景使用。

   3. 共享代价和批次（*Share expenses, batch*）

      - [集成层处理](https://wiki.c2.com/?IntegratedLayerProcessing)（*Integrated layer processing*）

        > 因为把协议软件分层会导致性能降低，层之间不能容易地分享信息，这些层可能记录了冗余信息，协议头就会变大。因此可以选择把这些层整合成一层。RTP 协议是以这个原则设计的。但是这样直接打破了网络体系结构，我觉得可能是可以的，但是我不好说。关于翻译，由于多层共享同一个报文，处理报文的代价是共享的，处理时段是相同的，是谓共享代价和批次。

3. 放松系统需求（*Relax system requirements*）

   1. 用确定性换时间（*Trade certainty for time*）

      - [随机公平队列](https://www.researchgate.net/publication/2411061_Stochastic_Fairness_Queuing)（*Stochastic fair queueing*）

        > 我们实际上不太在乎确定性，发包快就行了，不必指定哪个会话用哪个发送队列。

   2. 用准确性换时间（*Trade accuracy for time*）

      - 交换机负载均衡（*Switch load balancing*）

        > 不懂为什么会失去准确性……并且也没搜到这是个啥，只有 [Load-balanced switch](https://en.wikipedia.org/wiki/Load-balanced_switch)。

   3. 将计算变为空间（*Shift computation in space*）

      - [IPv6 分片](https://en.wikipedia.org/wiki/IP_fragmentation)（*IPv6 fragmentation*）

        > 这个原则的翻译我不好说，关于 IPv6 分片的事情可以看看[皮鞋哥](https://blog.csdn.net/dog250/article/details/88308917)。IPv6 禁止中间节点设备对 IP 报文进行分片，分片只能在端到端进行。中间设备只管转发就行了，不需要进行计算，但是这样空间就上去了。关于这个应该指的是用空间换时间，但是对不上原则 2.，所以就原样翻译了。这里的空间不指内存，而是指报文长度。

4. 充分利用系统组件（*Leverage off system components*）

   1. 利用局部性（*Exploit locality*）

      - 局部性驱动的接收器（*Locality-driven receiver*）

        > 我先猜一个空间局部性，C 语言黑魔法系列又来了。但是关于这个接收器是啥搜不到……

   2. 用空间换时间（*Trade memory for speed*）

      - [Luleå algorithm](https://en.wikipedia.org/wiki/Lule%C3%A5_algorithm)（*Processing; Lulea IP lookups*）

        > 可能是打错了，这里应该指的是这种 IP 查询的算法。看上去主要用压缩 Trie 实现的。并且这里是真正的算法里说的用空间换时间，和原则 3.3. 是不同意思的。

   3. 使用已有的硬件（*Exploit existing hardware*）

      - [快速 TCP 校验和](https://learn.microsoft.com/en-us/windows-server/networking/technologies/hpn/hpn-hardware-only-features#address-checksum-offload)（*Fast TCP checksum*）

        > 意思是把 TCP 算校验和交给网卡去做，能省 CPU 时间。

5. 添加硬件（*Add hardware*）

   > 这里书里可能有 typo，hardware 多了一个 g，第一条的 pipelining 少了一个 g。

   1. 使用内存交叉存取和管线化（*Use memory interleaving and pipelining*）

      - 管线化的 IP 查询（*Pipelined IP lookups*）

        > 倒是有[论文](https://link.springer.com/content/pdf/10.1007/978-3-642-03700-9_16.pdf)，但是没看出来用了什么新硬件，倒是设计了一个 bitmap。

   2. 使用宽字并行（*Use wide word parallelism*）

      - 共享内存交换机（*Shared memory switches*）

        > 这是啥玩意……

   3. 有效结合 DRAM 和 SRAM（*Combine DRAM and SRAM effectively*）

      - 维护计数器（*Maintaining counters*）

        > 因为 DRAM 和 SRAM 的速度和造价有很大差别。DRAM 造价低，但是存储速度慢，SRAM 造价高，存储速度快。相关问题复习计组，小丑竟是我自己。

6. 创建高效专用过程（*Create efficient specialized routines*）

   - UDP 校验和（*UDP checksums*）

     > UDP 校验和与 TCP 校验和计算方法是类似的，TCP 校验和链接里也说了可以算 UDP 校验和，不太明白为啥单挑这个例子讲。

7. 避免不必要抽象（*Avoid unnecessary generality*）
   - [Fbufs](https://dl.acm.org/doi/10.1145/173668.168634)

8. 不要被参考实现束缚（*Don't be tied to reference implementation*）
   - [Upcalls](https://dl.acm.org/doi/10.1145/323647.323645)

9. 在层接口中传递提示（*Pass hints in layer interfaces*）
   - 包过滤器（*Packet filters*）

10. 在协议头传递提示（*Pass hints in protocol headers*）

    - 标签交换（*Tag switching*）

      > 可能指的是 MPLS

11. 优化期望的情况（*Optimize the expected case*）

    - 头部预测（*Header prediction*）

      > [RFC 1185](https://www.rfc-editor.org/rfc/rfc1185) 的 2.3.2. 点提到了，但是里面引用的论文找不到了。

    1. 使用缓存（*Use caches*）
       - Fbufs

12. 添加状态以提升速度（*Add state for speed*）

    - 活跃虚电路列表（*Active VC list*）

      > 互联网上好像就没这个词一样……

    1. 增量计算（*Compute incrementally*）

       - 重新计算 CRC（*Recomputing CRCs*）

         > CRC 是可以增量计算的，后续的影响可以直接补到前面，不需要重复算以前的部分。

13. 优化自由度（*Optimize degrees of freedom*）
    - Trie 上的 IP 查询（*IP trie lookups*）

14. 使用桶排和位图（*Use bucket sorting, bitmaps*）
    - 时间轮（*Timing wheels*）

15. 建立高效的数据结构（*Create efficient data structures*）
    - 四层交换（*Level-4 switching*）

## 第一部分：游戏规则

### 第一章 网络算法学简介

网络算法学和传统算法不同，网络算法学认识到采取跨学科的系统方法来优化网络实现这一要点，而不是只用优化算法就好了。

网络算法学涉及操作系统（加速服务器），硬件设计（加速如路由器的网络设备）和算法设计（设计可扩展的算法），并且是一个系统方法。

网络算法学解决的问题是基础网络性能瓶颈，网络算法学所倡导的解决方案是一套解决这些瓶颈的基本技术。

#### 问题：网络瓶颈

本书的核心问题是如何让网络易用的同时实现裸硬件的性能。易用来自于强大的网络抽象，比如 socket 接口和基于前缀转发。但是如果不小心实现的话，这种抽象会产生比直接用光缆传更大的性能开销。

为了研究这个 gap，我们需要考虑两种网络设备：终端（*endnodes*）和路由器（*routers*）。

##### 终端瓶颈

终端包括 PC 和服务器，总之是网络的终端节点。终端是专门针对计算的，而不是网络，通常被设计为支持通用计算。因此终端瓶颈有两个主要原因：结构和规模。

- 结构（*Structure*）：为了运行任意代码，终端机的操作系统通常作为程序和硬件的媒介。OS 通常是分层的（比如有 HAL，资源管理层等等）；OS 实现了一些保护机制（*protection mechanisms*），以防应用程序破坏；最后，核心 OS 进程（也就是内核态），比如调度器和内存分配器是用通用机制（*general mechanisms*）写的，这个机制面向尽可能广泛的程序。但是 OS 分层，保护机制和过度抽象可能严重拖慢网络程序，即使在最快的处理器上跑也会很慢。
- 规模（*Scale*）：提供 Web 和其他服务的大服务器的出现导致了更深远的性能问题。比如 C10K，C10M 等问题。许多 OS 使用了低效的数据结构和算法，因为这些数据结构和算法是为连接数很少的时候设计的，太老了。

> 操作系统的结构和用户规模分别为内因和外因，同时制约网络性能。

我们将这些问题总结一下，如下表所示。

![](/images/network-algorithmics/f.1.1.png)

##### 路由器瓶颈

路由器是针对网络环境设计的，因此结构的 overhead 很小，OS 十分轻量并且专为路由器定制，很多东西都用硬件实现。取代结构，路由器瓶颈有两个主要原因：规模和服务。

- 规模（*Scale*）：同样有两方面规模
  - 带宽规模（*bandwidth scaling*）：光缆的传输速率越来越大，并且新应用程序越来越多，Internet 上的流量也越来越大了；
  - 网络规模（*population scaling*）：终端越来越多了。
- 服务（*Services*）：Internet 提供的服务越来越多，提供网络保障（*network guarantees*）变得十分重要——拥塞中时延保障，安全保障和故障发生时的可用性保障。

问题总结如下表所示。

![](/images/network-algorithmics/f.1.2.png)

#### 技术：网络算法学

当人们认识到 Internet 是一个由路由器和链路组成的系统时，可能就忽视了每个网络设备，比如从思科的 GSR（是思科的一款骨干网路由器）到 Apache 网络服务器都是系统。系统是由相互关联的子系统构建而成的，这些子系统在不同的时间点上被实例化。比如，一个核心路由器由带有转发引擎的[线卡](https://en.wikipedia.org/wiki/Line_card)和被纵横式交换机连接的数据包内存组成。路由器的行为受到各种时间的影响，其范围从制造时间（当默认参数存储在 [NVRAM](https://en.wikipedia.org/wiki/Non-volatile_random-access_memory) 中时）到路由计算时间（当路由器协商计算路由时）到数据包转发时间（当数据包被发送到相邻的路由器时）。

> 事物是普遍联系的。

因此用系统方法的一个重要的观察是通常可以通过在空间上（即在其他子系统上）或在时间上（即在明显需要该功能之前或之后的时间点上）移动它的一些功能来设计一个有效的子系统。

> 这句话完全没有理解什么是在时间或空间上移动功能。原文如下：
>
> Thus one key observation in the systems approach is that one can often design an efficient subsystem by moving some of its functions in space (i.e., to other subsystems) or in time (i.e., to points in time before or after the function is apparently required).

从某种意义上说，网络算法学的实践者是一个不择手段的机会主义者，愿意在任何时候改变规则以使游戏更容易。唯一的约束是整个系统所提供的功能要继续满足用户。

> 解决不了问题就换个问题，这是对的。解决问题可以用技巧，但是也可以靠力量。

考虑网络实现者在高速网络下面对的限制——越来越复杂的任务，需要支持更大的系统，小容量的高速内存和更少的内存访问次数——可能需要用尽实现技巧去跟上互联网日益增长的速度和规模。但是设计者可以直接加硬件，改变系统假设，设计一个新算法——反正任务是完成了。

#### 热身训练：鉴别恶意数据包

> 这节书里是在上一节之下的，但是后续跟它同级的内容都从属与它，所以单将其设置为一个大标题，后续内容作为这一节下的内容。

考虑一个前端网络监视器（就当它是个防火墙吧），要设计一个标记入流量中的可疑包——这种包可能进行缓冲区溢出攻击，攻击者在网络头部 $F$ 中放入机器码 $C$。

> 上学期刚做完缓冲区溢出实验这回又要防……

攻击主要原理就是缓冲区溢出导致注入的机器码（也可以是 shellcode）被执行。

> 其实未必要在防火墙层面上防，直接在设备上防开销已经不大了。

具体检查和防范的方法是，检测 URL 里包含的字符，因为要利用这个问题，势必 URL 会很长，并且大部分都是特殊符号（毕竟 shellcode 中大部分字符需要转成 URL 编码），比如 `#`。因此防火墙可以拦下这类包再进行检查。

当然有可能一些 CGI 脚本会被拦下，导致假阳性（*false positive*）的情况出现，因此我们假设有一个由安全工程师设计的规范被交给了芯片工程师。

这个例子是由 Mike Fisk 提出的问题。在面对这个问题时，我们从[稻草人设计](https://www.techopedia.com/definition/14585/straw-man)开始，一步一步优化这个设计，展示网络算法学的原则和技巧。

##### 直观方案

芯片维护两个字符类型数组 $T$ 和 $C$，每个数组的长度都是 $256$。门限数组 $T$ 包含每个字符可接受的百分比（也就是这种字符个数占全 URL 长度的最大占比）。如果某种字符的出现频率比这个值大，就说明这个包有问题。每个字符的门限限制可能不同。

$C$ 记录每个字符出现多少次，不想过多解释了。

所以整个算法过程就可以描述出来了。先扫一遍 URL，可以得到 $C$ 数组和字符串长度 $L$。然后对于每种字符，如果存在 $C_i\ge L\cdot T_i$，那么就把这个包标记可疑，否则放行。那么这个算法就是线性的，反正就扫扫完事了。

假设数据包高速进入路由器，我们希望在下一个数据包到达前把这个数据包处理完，这个要求叫线速处理（*wire-speed processing*），这个要求在网络中很常见，即使在最坏的情况下，它也能防止处理工作的积压。为了满足线速，理想的芯片设计需要对于每个 URL 中的字节做尽可能少的处理。假设主要步骤中一次自增操作可以在收到一字节之前完成。

> 因为自增操作十分快，收到一字节需要进行信号转换，但是自增操作过 CPU 的速度几乎是瞬间的。

但是这个方案中，我们扫了两次数组，首先对数组清零，然后检查是否超过门限限制。最小的数据包大小经常大概 $40$ 字节，并且仅包括网络头部。加上 $768$ 次操作（对 $C$ 中的每个元素的一次读，一次写和对 $T$ 数组所有元素的一次读），这个设计是不可接受的。

##### 想想算法

直观的说，第二次扫 $C$ 和 $T$ 是浪费的。比如，如果扫到一个字符超过限制了，直接返回就完事了，也就是常说的扫不满。这暗示了我们只需要维护出现次数的最大值就好了。但是这个方法是错的，因为在动态变化中不满足局部最优解是全局最优解，不能贪。

聪明的读者一定发现了，我们要判断的东西 $C_i\ge L\cdot T_i$ 这个不等式两端都有 $i$，但是我们换个形式：$\frac{C_i}{T_i}\ge L$，这样只有一端有 $i$ 了。实际上我们只需要维护 $\frac{C_i}{T_i}$ 的最大值即可。因为 $T_i$ 是常数，$C_i$ 自增，所以直接维护这个分式的最大值即可。正确性蕴含自之前的描述。

这样我们砍掉了第二次扫描，虽然操作次数没变，但是我们还是砍掉了一次扫描。

> 到这里的思路是非常正常的，下面就是黑魔法时间了。

##### 细化算法：利用硬件

我们优化了一次扫描，但是引入了除法。众所周知除法的电路逻辑十分复杂，算一次除法的周期比算加法长得多，我们怎么优化呢？

首先我们可以干掉浮点数，比如 $2.78\%$ 和 $3\%$ 差不多，那么门限百分比里的所有数都可以 round 到一个离它最近的整数上。那么既然都变成 $3\%$ 了，为什么不搞成 $\frac{1}{x}$ 这种形式呢，用百分数算完整数除法之后最后还得乘 $100$，用 $\frac{1}{x}$ 只需要乘 $x$。并且这个 $x$ 最好是 $2$ 的整数次幂，毕竟这样只需要移位就可以实现乘法操作了。

> 这里每一次转化都很合理，但是放在一起可能有些不可理喻，但这就是黑魔法。

因此逻辑电路只需要实现移位和比较电路就可以了。状态存储只需要一个寄存器存 $\frac{C_i}{T_i}$ 的最大值，但是还是没有优化读取和写入的操作次数。

最快的芯片内部存储需要 1 到 2 纳秒读一次，但是更慢的内存可能需要 10 纳秒左右——这比逻辑电路计算的时间要慢。单门延迟在皮秒量级内，位移逻辑的门延迟时间并不高。所以瓶颈在内存访问上。

我们可以把 $C$ 和 $T$ 两个数组合并成一个数组来减少内存访问，让两次读变成一次读。这是一个压位技巧，$C$ 数组是整数数组，每个整数设计为 $15$ 位长（这样可以处理不超过 32K 的数据包，考虑极限情况），$T$ 数组为了精度，每个整数设计为 $14$ 位长。因此这两个数组就被合成一个了，每个元素的位长为 $29$。实践上，硬件可以处理长达 $1000$ 位的宽字。同样的，把这两个数组分开也很简单，位操作一下就可以了。

> 因为位操作比访存快，所以我们确实可以优化速度。

##### 清理

说实话到这儿确实看起来不错，但是我们忘了一个最棘手的问题：我们忘了初始化 $C$ 数组。

初始化最大值为 $0$ 是对寄存器操作，这个操作是很快的，但是初始化 $C$ 数组又需要访存，我们不能承受这个访存时间。

这时我们可以考虑惰性计算，反正我只关心 $C$ 数组中跟这次计算有关的字符，无关的字符可以不用动。那么考虑记录一个 $3$ 位的版本号 $G$，每次需要自增一个 $C_i$ 时，先判断这个位置的版本号是否与当前包版本号相同，如果不同的话意味着这个位置要被初始化，并且是这个版本第一次访问到，因此需要置 $G_i$ 为数据包当前版本号，并初始化 $C_i$ 为 $1$，否则 $C_i$ 自增。之后的更新最大值操作仍然如同之前。

> 维护版本号并进行惰性初始化维护也很常见于各种算法竞赛的题目中，通常就是对一个很长的数组初始化很少的位置，直接 for 过去铁超时，所以用这种初始化。

由于版本号只是一个 $3$ 位的整数，因此可以想见这个版本号是在模 $8$ 意义下的，所以可以看成一种 Hash，可能出现版本号冲突，这样初始化就没有依据了。

这里考虑芯片设计中加入一个「擦除」循环，循环中读取这个数组，并且初始化所有过期的版本号。为了正确性，芯片必须保证在每八个数据包处理完之后进行这样的一次完整循环。考虑如果每个包有 $40$ 个非 URL 字节（除了 URL 之外有 $40$ 字节），八个包就是 $320$ 个，$320>256$，这段时间可以让数组的每个元素都被访问一次（读一次和写一次）。当然，这个非 URL 字节数是估计的，设计者真正可以获得的这个数值更大，通过增大版本号可以充分利用非 URL 字符产生的间隙，但是这样数组存储的空间就会增大。

> 这里有个假设，就是数据包都要过内存，也就是数据包要先存在内存里然后再被读取发出去。如果直接电路交换出去了，这个均摊是不成立的。但是应该确实所有包都要过内存的，毕竟要过转发队列的。
>
> 其实最简单的一种实现方式是：处理 8 个包后，芯片直接进入擦除态，这段时间里去访存擦除，但是这样是会形成时间毛刺的，这段时间没法处理包，虽然从数学意义上均摊了，但是不符合需要达到线速的要求。我们要把这段时间实际均摊到擦除过程中，而不是数学意义上。

那么芯片有两个状态：一个是处理 URL 的，一个是处理非 URL 的。当 URL 全部处理完后，芯片进入擦除态。芯片需要再维护一个寄存器 $s$ 表示下一个要擦除数组的哪个位置，当接收到一个非 URL 字节时，芯片判断如果 $G_s\neq g$（$g$ 为当前版本号），那么 $G_s$ 被置为 $g$，$C_s$ 置为 $0$。

> 我们可以保证每 8 个包经过后每个位置都被这样重置一遍，所以每个位置记录的 $G_i$ 可以认为都属于同一批 8 个包之内的，正确性是有保证的。

这样，我们用了 $32$ 位（$3$ 位 $G_i$，$15$ 位 $C_i$ 和 $14$ 位 $T_i$ 压一起）元素，长为 $256$ 个元素的数组就完成了这个操作，同时我们需要维护一个当前版本号 $g$，一个下一次要擦除的位置 $s$ 和 URL 长度 $L$，这些变量只需要用三个寄存器就可以维护了，代价很小。

> 黑魔法部分终于结束了。

#### 网络算法学的特点

- 网络算法学是跨学科的
  - 跨学科的思维有助于产生出最好的设计

- 网络算法学认识到了系统思维的重要性

  - 放宽要求（把门限百分比变成 $2$ 的幂次）和将工作从一个子系统迁移到另一个子系统是极其常见的系统技术，但是教学上不教

    > 这玩意过于 tricky 了，当然不会教。

  - 「黑盒思维」不利于产生出整体或系统思维

    > 毕竟我们在优化这个黑盒。

- 网络算法学从算法思维中获益

  > 有一说一算法部分到变换到 $\frac{C_i}{T_i}$ 的地方就停了，剩下的都是黑魔法，并且一般来说，这种变换也不叫新算法，算法中的 trick 罢了。

#### 网络算法学的定义

> Network algorithmics is the use of an interdisciplinary systems approach, seasoned with algorithmic thinking, to design fast implementations of network processing tasks at servers, routers, and other networking devices.

网络算法学运用跨学科的、系统的方法加上算法思维，为服务器、路由器和其它网络设备上的网络处理任务设计快速的实现。

> 上述中文定义摘抄自中科大网络算法学课件。

#### 练习

设计一个卡方检验的有效实现。也就是计算
$$
\sum_{i=1}^n\frac{(f_i-a_i)^2}{f_i}
$$
其中 $f_i$ 是预期频率，$a_i$ 是实际观察到的频率。如果这个值比一个门限值大，就需要告警。假设你只能在结尾获得长度（还是那个 URL 检测的例子）。

> 实际上还是一个路子，只不过上面变平方了需要化简
> $$
> \sum_{i=1}^n\frac{(f_i-a_i)^2}{f_i}=\frac{1}{L}\sum_{i=1}^n\frac{(T_i-C_i)^2}{T_i}\ge P
> $$
> 其中 $L$ 是长度，$P$ 是门限。$T_i$ 是预期次数，$C_i$ 为实际次数。
>
> 然后接着拆
> $$
> \frac{1}{L}\sum_{i=1}^n\frac{(T_i-C_i)^2}{T_i}=\frac{1}{L}\sum_{i=1}^nT_i-2C_i+\frac{C_i^2}{T_i}\ge P
> $$
> 常数挪右边去，不等式变为
> $$
> \sum_{i=1}^n \frac{C_i^2}{T_i}-2\sum_{i=1}^n C_i\ge PL-\sum_{i=1}^n T_i
> $$
> 考虑我们维护的是左边部分的和，$C_i$ 每自增 $1$，对左边和式的贡献是 $\frac{2C_i+1}{T_i}-2=\frac{2(C_i-T_i)+1}{T_i}$，所以就差不多一样维护就行了，每次对和加上这个贡献。只不过这个题里维护和，例子里维护最大值，并且设计时需要考虑整数是带符号的，需要设计有符号除法移位。

### 第二章 网络实现模型

为了改善终端和路由器的性能，实现者必须知道游戏规则。网络算法学的中心难点包括以下四方面：协议，硬件结构，操作系统和算法。

但是做算法的不懂硬件，做硬件的也不懂协议，如果然他们合作的话会导致沟通困难。如果想要实现人员之间的有效沟通，可以从使用简化模型（*simple models*）开始，这个模型具有解释和预测能力，但没有不必要的细节。这个模型至少应该定义书中使用的术语，这个模型最好使得一个不属于此领域的创作者可以和属于此领域的创作者一起共同生产和设计。例如，一个硬件芯片的实现者应该能够对芯片驱动的软件修改提出建议，而一个理论计算机科学家应该能够想出 switch arbitration 的硬件匹配算法。

> The switch arbitration problem is the problem of matching the inputs to the outputs.
>
> 应该可以翻译成交换仲裁

#### 协议

##### 运输和路由协议

应用程序把可靠传输的任务交给了 TCP。TCP 的工作是为发送和接收应用程序提供发送和接收的两个共享数据队列的模拟——尽管发送方和接收方机器被一个有损的网络分开。因此无论发送方无论向本地 TCP 发送队列里写了什么，接收方的接收队列里就应该按同样的顺序出现什么，反之亦然。TCP 通过报文分段和 ACK 机制实现，具体状态机可以参考下图。

![](/images/network-algorithmics/tcp_mech.png)

如果这个应用程序是视频会议程序，那么它会使用 UDP。

TCP 和 UDP 这样的传输协议通过从一个发送节点，跨 Internet 地将报文段发送给接收方的方式工作。实际上发送报文段的工作是 Internet 路由协议 IP 承担的。

Internet 路由分为两个部分，一个是转发（*forwarding*），一个是路由（*routing*）。

> 用熟悉的话讲，一个是数据平面，一个是控制平面。

转发必须在极高的速度下完成，转发表必须通过一个路由协议建立，尤其是在面对拓扑改变时，比如链路故障。有一些常用的路由协议，比如距离向量（RIP），链路状态（OSPF）和策略路由（BGP）。

##### 抽象协议模型

协议是所有参与协议的节点的状态机，同时还有接口和消息格式。

> A protocol is a state machine for all nodes participating in the protocol, together with interfaces and message formats.
>
> 更通俗的解释：协议由以下三部分组成：
>
> - 语法
> - 语义
> - 同步

对协议的定义中，必须描述这个状态机如何改变状态和产生对接口调用，收到的消息和定时事件的应答（例如通过发送消息或设置定时器的方式）。

![](/images/network-algorithmics/f.2.1.png)

本书致力于协议实现，所以，抽象出协议状态机进行的通用且耗时的函数（*generic and time-consuming functions*）是值得的。如下模型将贯穿全书。

![](/images/network-algorithmics/f.2.2.png)

首先，在上图的下方，一个协议状态机必须接受和发送数据包。这涉及数据处理（*data manipulations*），或者说数据包里的每个字节都需要被读或写。比如，TCP 必须把接受的数据拷贝到应用缓冲区里，路由器需要把交换包从输入链路交换到输出链路上。TCP 头部中还有一个校验和字段，算校验和必须遍历包的每一个字节。

在图的上方，状态机必须向诸多客户端之一解复用数据。在某些情况下，客户端程序必须被唤醒，这可能需要潜在的高代价控制传输。比如，当 TCP 接到了一个网页，它需要使用端口号向浏览器解复用网页数据，这可能需要唤起启动浏览器的进程。

这张图同样包含了一些由很多协议共享的通用函数。首先，协议的关键状态都应该可以被高速查询，有时也需要高速控制。比如 TCP 收到包了，就引发 TCP 查询连接状态表，同样，收到了 IP 包就导致了 IP 去查转发表。其次，协议需要有效地设置定时器，比如 TCP 的重传机制就需要定时器。第三，如果一个协议模块为多个客户端处理数据，它就需要有效编排这些客户端。比如 TCP 需要调度不同连接的进程，路由器必须保证一些对计算机之间的通信不会锁住其他通信。一些协议允许大数据包分段，这就涉及分段重组。

虽然这些通用函数通常代价很高，但是他们的开销可以通过正确的技术来缓解。

##### 性能环境和测量

本节介绍了一些重要的度量指标和性能假设。考虑一个系统（比如一个网络或者单独一个路由器），任务（比如消息）到来，处理和离开。在网络中最重要的两个度量指标是吞吐量（*throughput*）和延迟（*latency*）。吞吐量粗略地描述了每秒完成的任务数。延迟刻画完成一项任务的时间（通常是最差状况下的）。系统拥有者（比如 ISP）寻求最大化吞吐量以求最大化收入，同时系统用户想要端到端延迟小于几百毫秒。延迟同样影响了网络上的计算速度，比如 RPC 的性能。

在考虑实施权衡时，以下与互联网环境有关的性能观察是有帮助的。

- 链路速度：骨干网络速度已经升级到 10Gbps 和 40Gbps，本地链路也有 1Gbps。然而，无线网和家用链路要慢几个数量级。

  > 现在已经可以向运营商购买 100Gbps 的商用专线了，但是高吞吐量的家用链路并没有普及，而且很贵。无线网的带宽有所提升，但是提升有限。

- TCP 和 Web 的支配地位：Web 流量占 70%（字节数或者包数）。相似地，TCP 流量占 90%。

  > 虽然举的是 1998 年研究的例子，但是运营商确实偏爱 TCP，对 UDP 连接时常 QoS，目前仍然是 TCP 作为主导流量。

- 传输量小：大部分访问的 Web 文档很小，比如一个 1996 年的研究显示 50% 访问的 Web 文件都不超过 50KB

  > 这个研究可能旧了，现在 Web 技术日新月异，JS 大脚本会占用很多带宽，并且现在的带宽应该大部分是视频带宽了。

- 高延迟：实际的 RTT 已经超过了光速延迟，由一个 1995 年的研究，一个跨越美国的链接平均延迟为 241 毫秒，已经超过了光速延迟 30 毫秒。增长的延迟可能是由于提高吞吐量导致的，比如 modem 的批量压缩和路由器的管线化。

  > 猫已经没了……并且通过一些努力，比如 CDN，访问延迟高的情况正在改善。

- 局部性差：对骨干网流量的研究显示 25 万个不同的起点终点对（也称为一条流）在非常短的时间内通过路由器。最近的估计显示大约有一百万条并发流量。将具有相同目的地地址的报头组聚集起来或其他方式并不能显著减少报头类别的数量。因此局部性，或者说在一个数据包中投入的计算在未来的数据包中被重复使用的可能性很小。

  > 这里暗示网络流量的随机性很强。

- 小包：一项 1997 年的研究显示路由器收到的近半数的包是大小为最小大小 40 字节的 TCP ACK。为了避免小包丢包，大多数路由器和网络适配器制造商都致力于实现线速转发——这样就可以以输入链路的速度处理这些小包。

  > TCP 确实存在小包泛滥的情况。

- 关键指标：值得区分全局（*global*）性能指标和局部（*local*）性能指标，全局的性能指标比如端到端延迟和带宽，局部的比如路由器查询速度。虽然全局性能对于整体网络性能十分重要，本书只聚焦于局部性能指标。实践上，本书聚焦于转发性能和资源（比如内存和逻辑电路）测量。

- 工具：大部分网络管理工具聚焦于全局指标，比如惠普的 OpenView。本地测量所需的工具可以测量计算机内部软件的性能，比如做 profiling 的。例子有 Rational 的 Quantify，Intel 的 VTune 甚至硬件示波器。tcpdump 也很有用。

  > 看 Rational 就犯 PTSD，属于被软工折磨的。
  >
  > 但是 tcpdump 是真的有用。

> 中科大的课件里还有三条组合分析，很有道理，摘抄于此：
>
> - 高速链路+大量小包
>   - 包速率很高，到达间隔短；
>   - 线速处理难度大：处理一个包的时间必须非常短
> - 高速链路+大规模并发流
>   - 数据局部性差，Cache 用不上（命中率低），消除访存瓶颈困难
> - TCP 流占主导+TCP 处理开销大
>   - 优化 TCP 实现很重要

本节有一个案例分析，体现了协议特性可能极大影响程序性能，例子是 SAN 和 iSCSI 的，但是过于老了，现在真没听说过这俩东西了。同样中科大课件里也没有，于是目前从略。

#### 硬件

正如链路接近 40Gbps 的 [OC-768](https://en.wikipedia.org/wiki/Optical_Carrier_transmission_rates#OC-768) 速度一样，40 字节的包必须在 8 纳秒内转发出去。在这个速度下，包转发一般直接由硬件实现，而不是在可编程处理器上实现。但是搞软件的是不可能参与这样的硬件设计和开发的。接下来的一些简单模型可以让你快速入门。

Internet 查询通常用组合逻辑电路实现，Internet 数据包储存在路由器内存里，然后路由器将其和交换、查询电路放在一起，就组成了路由器。因此我们从逻辑电路实现开始讲起，接着讲内存，然后结束于基于组件的设计。更多细节可以参考 [Introduction to VLSI Systems](https://ai.eecs.umich.edu/people/conway/VLSI/VLSIText/PP-V2/V2.pdf) 这本书，还有经典的计算机结构教材 [Computer Architecture: A Quantitative Approach](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1)。

##### 组合逻辑电路

快速过一下数字逻辑的知识就可以了。只提供非门，与非门和或非门，任何有 $n$ 个输入的布尔函数 $f(I_1,\ldots,I_n)$ 都可以被实现。并且经常进行逻辑电路简化的操作，至于技巧还是复习数字逻辑。

例子 1 是为了实现 QoS 而实现优先级编码器（PE）。假设有一个路由器维护 $n$ 个输出队列，队列 $i$ 的优先级高于队列 $j$ 当且仅当 $i<j$。路由器中的发送调度器必须按优先级顺序，从第一个非空的数据包队列中挑选数据包。假设如果队列 $j$ 是非空的，调度器维护的一个 bitmap $I$ 就有 $I_j=1$。问一个 $I$ 中下标最小的 $1$ 是哪一个这个问题可以用硬件完成。我们会在例子 2 中详细研究这个函数。

> 实际上就是数二进制后缀 $0$，直接被优化了。

##### 计时器和功率

要把一个 40 字节大小的包以 OC-768 的速度转发，在这个包上进行的任何网络操作必须在 8 纳秒以内完成。因此在任意逻辑电路路径上，从输入到输出的最大信号传输延迟不能超过 8 纳秒。为了保证这个限制，需要一个晶体管内的信号传输延迟模型。

粗略地说，每个逻辑门，比如与非门或非门，可以想象成一组（当输入值改变时）必须充电的电容和电阻，这样计算输出值。更糟的是，给一个输入门充电可能引起后续输出门给未来的输入门充电。因此对于组合逻辑电路，计算函数的延迟是沿最差路径给每个晶体管充放电的延迟之和。这样的路径延迟必须适合于最小的包的到达时间。除给电容充电的时间之外，另一个延迟是线路延迟。

> 这部分应该是数字电路的知识，只需要知道这个延迟十分小，180 纳米制程的 CPU 通过单个逆变器驱动四个相同逆变器的输出负载的延迟是 60 皮秒就可以了。

为电容充能也需要能量，并且满足公式 $P=CV^2f$。当新处理器降低电压和电容时，高速电路就必须提高时钟频率。类似地，并行化暗示更多的电容可以同时充电。因此高速芯片会产生大量的热，需要不寻常的制冷技术。ISP 和主机托管商是耗电大户。虽然我们的抽象排除了对功率权衡的理解，但意识到芯片和路由器有时有功率限制是好的。今天，一些实际的限制是在单个芯片上每平方厘米 30 瓦，在数据中心每平方英尺 1 万瓦。

> 现如今绿色计算越来越受到重视，这样的要求只会越来越多。

例子 2 要把这个优先级编码器用电路实现。有两种实现，第一种用 $\mathcal{O}(n^2)$ 个晶体管，在 $\mathcal{O}(1)$ 的时间算出来，第二个实现用 $\mathcal{O}(n)$ 个晶体管，在 $\mathcal{O}(n)$ 的时间算出来。实际上，使用 *look-ahead* 这种分治算法可以做到用 $\mathcal{O}(n)$ 个晶体管形成一棵完全二叉树，然后可以用 $\mathcal{O}(\log n)$ 的时间求出来。

##### 提高硬件设计的抽象等级

设计涉及到的晶体管数量可能超过一百万的网络芯片是十分耗时的。设计芯片也有一些技术。

然而，最重要的一点是，就像软件设计者复用代码一样，硬件设计者也复用常见的功能。

后面作者考虑了一个 74LS138 译码器和 74LS148 编码器的设计。在被数字逻辑狠狠地教育了一遍之后深感崩溃，毕竟只记住了一个 74181，而且上次看数字逻辑还是 2019 年的事情。

例子 3 考虑纵横式编排和可编程优先级编码器。例子 1 已经设计好了这个编码器。但是路由器里通常用的是增强版的可编程优先级编码器（PPE）。和之前一样，有一个 $N$ 位的输入 $I$，然后又有一个 $\log N$ 位的输入 $P$。PPE 需要计算满足 $j\neq P$ 且 $I_j=1$ 的第一个 $j$。如果 $P=0$ 就按 PE 工作。

> 但是囿于捉襟见肘的数字逻辑水平和快崩溃了的心情，PPE 具体实现部分从略。
>
> 真的，看这部分有时候低头想，有时候抓头，有时候气笑了，跟精神病一样。

##### 内存

在终端和路由器，包的转发用组合逻辑电路进行，但是包和转发状态存储在内存中。由于访存时间显著慢于逻辑电路延迟，内存形成了终端和路由器的主要瓶颈。

此外，不同的子系统对内存特性的要求不同。比如路由器厂商想要对包缓存 200 毫秒，这是 RTT 的上限，如果因为拥塞丢包了就从缓存里拿出来重试。如果在 40Gbps 的链路上，这样的缓存就需要大量空间。

> 大概需要 1GB 的缓存。

另一方面，路由器查询需要一块小内存，并且可以随机访问。这样对于不同的内存技术就需要一个简单的模型。

###### 寄存器

寄存器是由触发器组成的，触发器是两个或多个晶体管通过反馈环路组成的，这样就可以储存信号了。逻辑电路访问寄存器是十分快的，大概 0.5 纳秒到 1 纳秒。

> 依靠双稳态电路内部交叉反馈的机制存储信息。

###### SRAM

SRAM（*static random access memory*）由 $N$ 个寄存器组成，这些寄存都有一个长度为 $\log N$ 的地址。由于触发器自己刷新自己，所以是 static（静态）的。除了触发器，SRAM 还需要一个译码器执行选择寄存器的操作。片上 SRAM 的访问时间大概有 1 纳秒到 2 纳秒，片外 SRAM 的访问时间通常为 5 到 10 纳秒。

> CPU 中的 L1 到 L3 缓存也都是 SRAM，容量各有大小，但是 L1 大小最小，L3 最大。
>
> L1 缓存分为指令缓存 L1i 和数据缓存 Lid，和 L2 缓存为一个核心独享。L3 缓存是一个 CPU 中所有核心共享的。现在 Intel 搞的共享缓存是指 L3。

###### DRAM

一个 SRAM 中，存一个位要用至少 5 个晶体管。因此 SRAM 通常密度低并且很贵，通常做大容量内存的是 DRAM。SRAM 不需要刷新而 DRAM 需要定时刷新，几毫秒充一次电，但是 DRAM 存储密度高，结构简单并且造价低。DRAM 按行列组织存储单元，这样读取就需要经过片选，也就是行选和列选两个阶段。行列的编号规模是 $\mathcal{O}(\sqrt N)$ 的，这减低了解码门电路的规模。

注意连续的行选和列选之间有一个预充电延迟，这段时间需要给电容充电。

> 具体过程复习计组。

片上 DRAM 的访存延迟大约为 30 纳秒，最快的片外 DRAM 访存延迟为 40 到 60 纳秒，连续读的延迟约为 100 纳秒（由于预充电限制）。

###### 页模式 DRAM（Page-mode DRAM）

一般来说 DRAM，或者说内存都是分页存储的。中科大课件中的翻译是快页内存，快页内存是 Fast Page-mode DRAM，但是和这里说的情况貌似没啥区别。

因为我们发现很多数组遍历操作一下子读取一行，那么我们可以考虑把这一行的内容都缓存一下，这种性质就是局部性。下图展示了一个典型的查询过程。由于我们缓存了一行，因此 RAS 预充电过程可以省略，这段延迟就没有了。显存直接把一行内容读进 SRAM，这样就可以高速串行读出刷新显示器。

![](/images/network-algorithmics/f.2.5.png)

###### 交错 DRAM（Interleaved DRAMs）

内存延迟对计算速度有关键影响，同时内存吞吐量（通常称为带宽）也是十分重要的。假设一个内存的字大小是 32bit，一个周期是 100 纳秒。那么单次拷贝的吞吐量就限制在 32b/ns 这个速度。

> 大概是 3.725GB/s。现如今最新的 DDR5 的最大带宽大概在 70GB/s 左右。

显然，吞吐量可以使用访问多个 DRAM 来提升。在下图中，多个 DRAM（被称为 bank）被一条总线连在一起。

![](/images/network-algorithmics/f.2.6.png)

用户可以先把对 Bank 1 的读操作放在总线上。假设每个 DRAM 返回所选的数据需要 100 纳秒。那么在等待的这 100 纳秒中，用户可以把对 Bank 2 操作的地址放在总线上，以此类推。如果放一个地址需要 10 纳秒，那么在等 Bank 1 的结果的同时我们可以进行 10 个操作，这样就可以有效利用空闲时间而提高 10 倍的带宽，只要用户连续访问不同的 bank 就可以了。

目前的产品有 SDRAM（*Synchronous DRAM*，进化成了后来的 DDR）和 RDRAM（*Rambus DRAM*），SDRAM 集成了两个 bank，RDRAM 集成了 16 个 bank，但是 RDRAM 必须成对使用，现在已经退出市场了。

例子 4 考虑管线化流 ID 查询。一条流用源和目的的 IP 地址和 TCP 端口号来确定（也被称为网络四元组）。一些用户希望路由器跟踪每条网络流的发包数量，这是为了审计。这就需要一个数据结构存对于每个流 ID 的计数器。并且需要支持两个操作：插入和查询。这里的查询是精确查询，并且需要包来了就查。

> 到这里，要求还是比较简单的。用哈希或者开个 map 统计一下就好。

但是，如果我们要求实现线速，比如考虑最差情况，在 2.5Gbps 的 OC-48 速度下，一个 40B 的包要查流 ID，只预留了 128 纳秒。并且根据 1997 年的一项研究，核心路由器上预估有 100 万条并发流。

我们考虑使用平衡树，比如 B-Tree。为了速度，理想情况下应该把这棵树存 SRAM 里，但是要存 100 万个流，至少要 $\frac{10^6\times96/8}{2^{20}}=11\text{M}$，这个存储代价太大了。但是用 DRAM 的话，就会产生最差 $\log_2 10^6\approx 20$ 次访存，考虑一次访存最快 50 纳秒，那么整个查询就需要 1 微秒，这太慢了。

一个解决方案是用管线化，我们用一个 16 个 bank 的 RDRAM 存这棵 B-Tree，第 $i$ 层的所有节点存储在 Bank $i$ 中。查询时并发进行 16 个流 ID 查询。对于 Packet 1，第一步在 Bank 1 里找，然后芯片可以在 Bank 2 中查 Packet 1 的第二层节点，在十分短的时间之后，可以查询 Packet 2 在 Bank 1 中的位置。当 Packet 1 在 Bank 16 中查询时，Packet 16 在 Bank 1 查询。因为 RDRAM 的运行频率为 800MHz，与读取访问时间 60 纳秒相比，两个地址请求之间时间很短。

![](/images/network-algorithmics/f.2.7.png)

> 啥是 the time between address requests，能从运行频率算出来一个周期的时间是 1.25ns。

因此先需要 $16\times 60$ 纳秒预热流水线，预热后每个查询都只需要 $60$ 纳秒。

> 这里只考虑终端什么时候会吐出来一个需要 +1 的地址。

不幸的是，一个 16 层的二叉树只能存 $2^{16}=65536$ 个流 ID，这太小了。由于 RDRAM 可以使用一种页模式的变种，可以同时读取 8 个 32 位的数据，时间和读一个差不多。这就允许我们同时读两个 96 位的 key 和三个 20 位的指针。因此可以使用一棵三叉的 B- 树，允许存储 $3^{16}$ 个流 ID，这样就满足要求了。

> 在想 B 树，B- 树和 B+ 树有啥区别。我有点怀疑 B- 树会不会是 B-Tree 翻译错了。

##### 内存子系统的设计技巧

- 内存交错和管线化
  - 相似的技巧用在了 IP 查询，分类和调度算法中。多 bank 的技术可以用多个外部存储来实现
- 宽字并行
  - 可以用 DRAM 的页模式或者字宽大的 SRAM
- 结合 DRAM 和 SRAM
  - SRAM 昂贵且快，DRAM 便宜且慢，所以要平衡它们各自的优势。比如 SRAM 作为 DRAM 的缓存。

##### 组件级别的设计

上两节中介绍的方法可以用于实现任意计算的状态机。一些关键芯片可能直接给路由器或者 NIC 设计，其余的设计就被称为组件级别的设计：比如组织和连接这些芯片，把板子放在盒子里的同时注意形状因数，电力和制冷。一个主要方面是理解针脚数限制，它通常提供一个快速的可行性检查。

例子 5 是考虑路由器缓存的针脚数。考虑一个有 $40$ 个 $10\text{Gbps}$ 连接的路由器，总缓存大小为 $200\text{ms}\times 400 \text{Gbps}=80\text{Gb}$。我们使用 DRAM。因为每个包必须进出一次缓存，缓存带宽应该是入带宽的二倍，也就是 $40\times 10\times 2=800\text{Gbps}$。为了留余量，我们设计的内存带宽是 $1600\text{Gbps}$。

使用 $16$ bank 的 64 字宽的 RDRAM，峰值内存带宽为 $10.24\text{Gbps}$。访问 RDRAM 需要 $64$ 个数据针脚和 $25$ 个其他针脚（地址或控制线），大概总共有 $90$ 个针脚。如果要达到 $1600\text{Gbps}$，大概需要 $160$ 个 RDRAM，这样就出现了 $14400$ 个针脚。一个芯片上针脚数保守上限差不多是 $1000$。这就暗示了即使路由器厂商想要制造一个极快的转发芯片，可以在最大速度下 handle 所有的包，它依然需要至少多一块芯片去驱动数据进出 RDRAM。

> 只看出来这种设计是有问题的，但是为什么只多一块芯片我不知道。因为肯定是要用 160 块 RDRAM 了，除非不用 RDRAM 否则不能避免这个 160 块的代价。

我们想说的就是针脚数是一个关键限制。

##### 硬件设计经验总结

这个经验总结到 2004 年可用的经验。

> 太老了，也就图一乐。

- 芯片复杂度扩展
- 芯片速度
- 芯片 IO
- 串行 IO
- 内存扩展
- 功耗和芯片封装技术

#### 网络设备结构

优化网络性能需要优化数据传递路径上的点。因此理解终端和路由器内部结构是十分重要的。
